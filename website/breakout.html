<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Breakout DQN PER Agent - Project Documentation</title>
    <!-- Load Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <style>
        /* Custom styles for a monochrome, high-contrast look */
        body {
            font-family: 'Inter', sans-serif;
            background-color: #ffffff; /* White background */
            color: #1f2937; /* Very dark gray text */
        }
        .container-border {
            border: 2px solid #1f2937; /* Strong black border */
            box-shadow: 6px 6px 0px 0px #9ca3af; /* Simple grayscale shadow */
        }
        .code-block {
            background-color: #1f2937; /* Dark background for code */
            color: #e5e7eb; /* Light gray text for contrast */
            border-radius: 0.5rem;
            padding: 1.5rem;
            overflow-x: auto;
            font-family: 'Consolas', 'Monaco', monospace;
            font-size: 0.9rem;
            border: 1px solid #9ca3af;
        }
        /* Ensure max-width on mobile doesn't break layout */
        .max-w-7xl {
             max-width: 100%;
        }
    </style>
</head>
<body class="antialiased">
    <div class="max-w-7xl mx-auto py-8 px-4 sm:px-6 lg:px-8">

        <!-- Header & Agent Demo Section -->
        <header class="text-center mb-12 sm:mb-16">
            <div class="inline-block p-4 container-border rounded-lg bg-gray-50">
                <h1 class="text-4xl sm:text-5xl font-extrabold mb-2">
                    BREAKOUT: DQN PER AGENT
                </h1>
                <p class="text-lg sm:text-xl font-medium text-gray-700">
                    Deep Q-Learning with Prioritized Experience Replay (PER)
                </p>
            </div>

            <!-- Demo Area -->
            <div class="mt-8 sm:mt-10 mx-auto max-w-xl container-border rounded-lg overflow-hidden p-3 bg-gray-100">
                <p class="text-sm font-semibold mb-2 text-center text-gray-800">AGENT PERFORMANCE DEMO</p>
                <!-- GIF Placeholder: Replace the src below with your animated GIF URL -->
                <img
                    src="breakout_5000.gif"
                    onerror="this.onerror=null; this.src='https://placehold.co/700x400/9ca3af/1f2937?text=PLACE+YOUR+BREAKOUT+AGENT+GIF+HERE';"
                    alt="DQN PER Agent playing Atari Breakout"
                    class="w-full h-auto object-cover container-border rounded"
                >
            </div>
        </header>

        <!-- Project Files and Links Section -->
        <section class="mb-12 sm:mb-16 mt-8">
            <div class="pb-2 mb-6 border-b-2 border-gray-800">
                <h2 class="text-2xl sm:text-3xl font-bold">
                    Project Files & Resources
                </h2>
            </div>
            <p class="text-base sm:text-lg text-gray-700 mb-4">
                View the full implementation, including network definitions and environment wrappers, in the complete Jupyter Notebook.
            </p>
            <!-- Placeholder Link to Jupyter Notebook -->
            <a href="DeepRL_breakout_modified.ipynb" target="_blank" class="inline-block px-6 py-3 bg-gray-800 text-white font-bold rounded-lg hover:bg-gray-700 transition container-border shadow-lg">
                View Full Jupyter Notebook ($\texttt{DeepRL\_breakout.ipynb}$)
            </a>
            <p class="text-xs mt-2 text-gray-500">
            </p>
        </section>


        <!-- DQN Code Analysis Section -->
        <section class="mb-12 sm:mb-16 mt-16 sm:mt-20">
            <div class="pb-2 mb-6 border-b-2 border-gray-800">
                <h2 class="text-2xl sm:text-3xl font-bold">
                    The DQN Agent with Prioritized Experience Replay
                </h2>
            </div>

            <p class="text-base sm:text-lg text-gray-700 mb-6">
                This DQN implementation utilizes **Prioritized Experience Replay (PER)**, where transitions with higher TD-error (greater surprise) are sampled more frequently. This accelerates learning by focusing on the most informative experiences.
            </p>

            <div class="code-block">
                <pre><code>/**
 * DQN Agent for Breakout using Prioritized Experience Replay (PER).
 * The ReplayBuffer must be a specialized SumTree structure to handle priorities.
 */
class DQNPrioritizedAgent {
    constructor() {
        this.model = this.createModel();
        this.targetModel = this.createModel();
        this.targetModel.setWeights(this.model.getWeights()); 
        
        // ... Hyperparameters (GAMMA, EPSILON, BATCH_SIZE) ...
        this.ALPHA = 0.6; // PER parameter: controls priority degree
        this.BETA = 0.4;  // PER parameter: controls importance sampling correction
        
        // this.memory = new PrioritizedReplayBuffer(MEMORY_SIZE); // Specialized buffer
        
        console.log("DQN PER Model initialized (TensorFlow.js simulation)");
    }

    /**
     * Stores an experience tuple (s, a, r, s', done) into the memory buffer.
     * Initially, the priority (p) is set to the maximum observed priority.
     */
    remember(state, action, reward, nextState, done) {
        // max_p = this.memory.getMaxPriority();
        // this.memory.add({ state, action, reward, nextState, done }, max_p);
    }

    /**
     * Trains the model using a batch sampled according to priority.
     */
    async learn() {
        if (this.memory.length < this.BATCH_SIZE) return;

        // 1. Sample batch and calculate Importance Sampling (IS) weights
        // const { batch, indices, weights } = this.memory.sample(this.BATCH_SIZE, this.BETA);
        // const states = tf.tensor2d(batch.map(m => m.state));
        // const nextStates = tf.tensor2d(batch.map(m => m.nextState));

        // 2. Calculate TD-error (Temporal Difference Error)

        // 3. Calculate Target Q-values (Standard DQN or DDQN rule)
        // ... (target calculation: R + gamma * max_a' Q'(s', a')) ...
        
        // 4. Fit/Train the main model
        // The loss is weighted by the IS weights to correct for biased sampling.
        // const loss = tf.losses.meanSquaredError(targetTensor, currentQ).mul(weights).mean();
        // await this.model.fit(states, targetTensor, { epochs: 1, batchSize: this.BATCH_SIZE, sampleWeight: weights, verbose: 0 });

        // 5. Update priorities in the buffer using the new TD-error (abs(target - prediction))
        // this.memory.updatePriorities(indices, new_td_errors); 
        
        // ... Epsilon decay ...
    }
    
    // ... createModel() and updateTargetModel() remain similar ...
}</code></pre>
            </div>
        </section>

        <!-- Prioritized Experience Replay (PER) Concept Section (REPLACED DUELING) -->
        <section class="mb-12 sm:mb-16 mt-12">
            <div class="pb-2 mb-6 border-b-2 border-gray-800">
                <h2 class="text-2xl sm:text-3xl font-bold">
                    Prioritized Experience Replay (PER) Concept
                </h2>
            </div>

            <p class="text-base sm:text-lg text-gray-700 mb-6">
                PER replaces uniform sampling by prioritizing experiences based on their magnitude of **Temporal Difference (TD) error**, $|\delta|$. High TD-error means the agent was "surprised," making that sample more valuable for learning.
            </p>

            <div class="code-block">
                <pre><code>// 1. Sampling Probability ($P_i$)
// $\alpha$ determines how much prioritization is used ($\alpha=0$ is uniform sampling).
$P(i) = \frac{(\text{TD-Error}_i + \epsilon)^\alpha}{\sum_k (\text{TD-Error}_k + \epsilon)^\alpha}$

// 2. Importance Sampling (IS) Weight ($\omega_i$)
// $\omega_i$ corrects the bias introduced by non-uniform sampling.
// $\beta$ anneals from a starting value (e.g., 0.4) to 1.0 over training.
$\omega_i = (\frac{1}{N} \cdot \frac{1}{P(i)})^\beta$

// The loss function is then weighted by $\omega_i$:
$\text{Loss} = \omega_i \cdot \text{MSE}(\text{Target}, Q(\text{State}, \text{Action}))$</code></pre>
            </div>
        </section>

        <!-- Training Code Snippet Section (IPyNB) -->
        <section class="mb-12 sm:mb-16 mt-12">
            <div class="pb-2 mb-6 border-b-2 border-gray-800">
                <h2 class="text-2xl sm:text-3xl font-bold">
                    Jupyter Notebook Training Initialization
                </h2>
            </div>
            <p class="text-base sm:text-lg text-gray-700 mb-6">
                The following cells initialize the PyTorch models, environment, and training components before starting the main $\texttt{train\_modif}$ loop. Note the environment ID change to $\texttt{BreakoutNoFrameskip-v4}$.
            </p>
            <div class="code-block">
                <pre><code>import torch
import torch.optim as optim
from model import DQN
from wrappers import make_atari_env
from replay_memory import ReplayBuffer // Note: This should be a PER implementation
from utils_modif import train_modif, test_modif
from Param_modif import *
from IPython.display import clear_output
import matplotlib.pyplot as plt
%matplotlib inline

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device
// Output: device(type='cuda')

env_id = "BreakoutNoFrameskip-v4"
env = make_atari_env(env_id)
print(env.action_space)
// Output: Discrete(4)
// Output: ['NOOP', 'FIRE', 'RIGHT', 'LEFT']

current_model = DQN(env.observation_space.shape, env.action_space.n).to(device)
target_model = DQN(env.observation_space.shape, env.action_space.n).to(device)
optimizer = optim.Adam(current_model.parameters(), lr=0.0001)
replay_buffer = ReplayBuffer(MEMORY_SIZE) // Assuming this is now a PER buffer
target_model.load_state_dict(current_model.state_dict())
// Output: &lt;All keys matched successfully&gt;

train_modif(env, current_model,target_model, optimizer, replay_buffer, device)
</code></pre>
            </div>
        </section>


        <!-- DQN PER Pseudo-code Section (UPDATED) -->
        <section class="mb-12 sm:mb-16 mt-12">
            <div class="pb-2 mb-6 border-b-2 border-gray-800">
                <h2 class="text-2xl sm:text-3xl font-bold">
                    DQN with PER Training Algorithm (Pseudo-code)
                </h2>
            </div>

            <p class="text-base sm:text-lg text-gray-700 mb-6">
                The core logic now involves calculating the Temporal Difference (TD) error to set the priority for new samples and to determine the Importance Sampling weight $\omega$ for training.
            </p>

            <div class="code-block">
                <pre><code>Initialize Q-network Q with random weights $\theta$.
Initialize target Q-network Q' with weights $\theta' = \theta$.
Initialize **Prioritized** Replay buffer D.

For each EPISODE:
  Initialize state $s$
  For $t = 1$ to $T$:

    // 1. Action Selection ($\epsilon$-Greedy Policy)
    Select action $a = \underset{a}{\operatorname{argmax}} Q(s, a; \theta)$ (or random w/ $\epsilon$)

    // 2. Environment Interaction
    Execute action $a$, observe reward $r$, and new state $s'$

    // 3. Calculate and Store Initial Priority
    // Calculate TD-error $|\delta|$ for this new sample.
    // Store experience $(s, a, r, s')$ in buffer D with priority $p \propto |\delta|^\alpha$.
    Set $s = s'$

    // 4. Training Step (Sampling and Optimization)
    // Sample mini-batch $(s_j, a_j, r_j, s'_j)$ with probability $P_j$ from D.
    // Calculate Importance Sampling weight $\omega_j$ for each sample.

    // 5. Calculate Target Value ($Y_j$)
    Calculate $y_j = r_j + \gamma \cdot \underset{a'}{\operatorname{max}} Q'(s'_j, a'; \theta')$

    // 6. Update Main Network & Priority
    // Calculate new TD-error: $\delta_j = y_j - Q(s_j, a_j; \theta)$
    // Update network weights $\theta$ using weighted gradient descent:
    Perform a gradient descent step on $\omega_j \cdot (\delta_j)^2$ to update $\theta$.
    // Update the priority $p_j$ in D using the new $|\delta_j|$.

    // 7. Update Target Network
    Every $C$ steps: $\theta' = \theta$.

  Decay $\epsilon$ and Anneal $\beta$
</code></pre>
            </div>
        </section>

        <!-- Training Performance Graphs Section (MODIFIED) -->
        <section class="mb-12 sm:mb-16 mt-12">
            <div class="pb-2 mb-6 border-b-2 border-gray-800">
                <h2 class="text-2xl sm:text-3xl font-bold">
                    Training Performance Metric (Episodic Reward)
                </h2>
            </div>
            <p class="text-base sm:text-lg text-gray-700 mb-6">
                The graph below illustrates the agent's performance by tracking the episodic reward curve over training steps.
            </p>

            <div class="mx-auto max-w-4xl">
                <!-- Graph 1: Reward History (Now full width) -->
                <div class="container-border rounded-lg overflow-hidden p-3 bg-gray-100">
                    <p class="text-sm font-semibold mb-2 text-center text-gray-800">EPISODIC REWARD HISTORY</p>
                    <img
                        src="break.png"
                        onerror="this.onerror=null; this.src='https://placehold.co/800x450/9ca3af/1f2937?text=REWARD+VS+EPISODE+GRAPH+(BIGGER)';this.className='w-full h-auto object-cover container-border rounded';"
                        alt="Graph showing episodic reward history"
                        class="w-full h-auto object-cover container-border rounded"
                    >
                </div>
            </div>
        </section>

        <footer class="text-center pt-8 border-t-2 border-gray-400 mt-12">
            <p class="text-gray-600 font-mono text-xs sm:text-sm">A Reinforcement Learning Documentation Project | Built with Simplicity and Contrast.</p>
        </footer>
    </div>
</body>
</html>