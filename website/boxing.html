<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Boxing DDQN Dueling Agent - Project Documentation</title>
    <!-- Load Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        /* Custom styles for a monochrome, high-contrast look */
        body {
            font-family: 'Inter', sans-serif;
            background-color: #ffffff; /* White background */
            color: #1f2937; /* Very dark gray text */
        }
        .container-border {
            border: 2px solid #1f2937; /* Strong black border */
            box-shadow: 6px 6px 0px 0px #9ca3af; /* Simple grayscale shadow */
        }
        .code-block {
            background-color: #1f2937; /* Dark background for code */
            color: #e5e7eb; /* Light gray text for contrast */
            border-radius: 0.5rem;
            padding: 1.5rem;
            overflow-x: auto;
            font-family: 'Consolas', 'Monaco', monospace;
            font-size: 0.9rem;
            border: 1px solid #9ca3af;
        }
        /* Ensure max-width on mobile doesn't break layout */
        .max-w-7xl {
             max-width: 100%;
        }
    </style>
</head>
<body class="antialiased">
    <div class="max-w-7xl mx-auto py-8 px-4 sm:px-6 lg:px-8">

        <!-- Header & Agent Demo Section -->
        <header class="text-center mb-12 sm:mb-16">
            <div class="inline-block p-4 container-border rounded-lg bg-gray-50">
                <h1 class="text-4xl sm:text-5xl font-extrabold mb-2">
                    BOXING: DDQN DUELING AGENT
                </h1>
                <p class="text-lg sm:text-xl font-medium text-gray-700">
                    Optimized Deep Reinforcement Learning with Dueling Architectures
                </p>
            </div>

            <!-- Demo Area -->
            <div class="mt-8 sm:mt-10 mx-auto max-w-xl container-border rounded-lg overflow-hidden p-3 bg-gray-100">
                <p class="text-sm font-semibold mb-2 text-center text-gray-800">AGENT PERFORMANCE DEMO</p>
                <!-- GIF Placeholder: Replace the src below with your animated GIF URL -->
                <img
                    src="boxing_500.gif"
                    onerror="this.onerror=null; this.src='https://placehold.co/700x400/9ca3af/1f2937?text=PLACE+YOUR+BOXING+AGENT+GIF+HERE';"
                    alt="DDQN Dueling Agent playing Atari Boxing"
                    class="w-full h-auto object-cover container-border rounded"
                >
            </div>
        </header>

        <!-- Project Files and Links Section (NEW) -->
        <section class="mb-12 sm:mb-16 mt-8">
            <div class="pb-2 mb-6 border-b-2 border-gray-800">
                <h2 class="text-2xl sm:text-3xl font-bold">
                    Project Files & Resources
                </h2>
            </div>
            <p class="text-base sm:text-lg text-gray-700 mb-4">
                View the full implementation, including network definitions and environment wrappers, in the complete Jupyter Notebook.
            </p>
            <!-- Placeholder Link to Jupyter Notebook -->
            <a href="DeepRL_boxing_modified.ipynb" target="_blank" class="inline-block px-6 py-3 bg-gray-800 text-white font-bold rounded-lg hover:bg-gray-700 transition container-border shadow-lg">
                View Full Jupyter Notebook "Mode performance Code"</a>
            <p class="text-xs mt-2 text-gray-500">
                
            </p>
        </section>


        <!-- DQN Code Analysis Section -->
        <section class="mb-12 sm:mb-16 mt-16 sm:mt-20">
            <div class="pb-2 mb-6 border-b-2 border-gray-800">
                <h2 class="text-2xl sm:text-3xl font-bold">
                    The Double DQN Dueling Agent (DDA) Code
                </h2>
            </div>

            <p class="text-base sm:text-lg text-gray-700 mb-6">
                This implementation uses a Double DQN update rule to combat Q-value overestimation, combined with a **Dueling Network Architecture (DDA)** for faster convergence. The state input is a preprocessed image/vector (4-tuple used for simplified demonstration).
            </p>

            <div class="code-block">
                <pre><code>/**
 * Double DQN Dueling Agent for the Boxing environment.
 * The core distinction is the Dueling architecture in createModel() 
 * and the Double DQN update logic in learn().
 */
class DoubleDQNDuelingAgent {
    constructor() {
        // Initializes the primary Q-network and the target network.
        this.model = this.createModel();
        this.targetModel = this.createModel();
        this.targetModel.setWeights(this.model.getWeights()); // Sync weights

        // Hyperparameters
        // ... (Hyperparameters remain similar to standard DQN)
        this.GAMMA = 0.99;
        this.BATCH_SIZE = 32;
        this.MEMORY_CAPACITY = 5000;
        this.epsilon = 1.0;
        
        console.log("DDA Model initialized (TensorFlow.js simulation)");
    }

    /**
     * Defines the structure of the Dueling Q-Network (DQN).
     * The final hidden layer is split into two streams: Value (V) and Advantage (A).
     * Input State: [Player X, Player Y, Opponent X, Opponent Y, ... (Atari image stack)]
     * Output: Q-values for all 18 Atari Boxing actions.
     */
    createModel() {
        // Placeholder for TensorFlow.js Dueling Network definition
        /*
        const model = tf.sequential();
        // Shared Layers (e.g., Convolutional layers for image input)
        model.add(tf.layers.dense({ units: 128, activation: 'relu', inputShape: [4] })); 
        
        // --- Split into two streams ---
        
        // Value Stream (V(s))
        const value_stream = tf.layers.dense({ units: 1, activation: 'linear' }); 

        // Advantage Stream (A(s, a))
        const advantage_stream = tf.layers.dense({ units: 18, activation: 'linear' }); // 18 is the action space size

        // Combine (Q(s,a) = V(s) + (A(s,a) - Avg(A(s,a))))
        // The final layer applies the Dueling mechanism logic.
        
        model.compile({ optimizer: 'adam', loss: 'meanSquaredError' });
        return model;
        */
        return {
            predict: (states) => { /* simulated prediction */ return { argMax: () => ({ dataSync: () => [1] }) } },
            fit: async (states, targets) => { /* simulated training */ },
            getWeights: () => [],
            setWeights: () => {}
        };
    }

    /**
     * Stores an experience tuple (s, a, r, s', done) into the memory buffer.
     */
    remember(state, action, reward, nextState, done) {
        // ... standard experience replay logic ...
    }

    /**
     * Double DQN Training Step (Sampling and Optimization).
     * Crucially, the target Q-value calculation uses the Double DQN rule.
     */
    async learn() {
        if (this.memory.length < this.BATCH_SIZE) return;

        // 1. Sample batch (states, actions, rewards, nextStates, dones)

        // 2. Calculate Target Q-values (Double DQN Update Rule)
        /*
        // Get action chosen by the *Main* Network Q for the next state s'
        const main_Q_actions = this.model.predict(nextStates).argmax(1);

        // Get Q-value estimate from the *Target* Network Q' using the action
        // selected by the Main Network Q. This breaks the Q-value overestimation bias.
        const next_Q_values = this.targetModel.predict(nextStates);
        
        // Next state Q-value estimation: Q'(s', argmax_a Q(s', a; theta); theta')
        // const next_max_Q = tf.gather(next_Q_values, main_Q_actions, 1); 

        // Target: R + gamma * next_max_Q
        // const target_Q = rewards.add(next_max_Q.mul(this.GAMMA).mul(dones.mul(-1).add(1)));
        */
        
        // 3. Create the updated target Q matrix for training
        // ... (standard DQN update process using the DDQN target)

        // 4. Fit/Train the main model
        // await this.model.fit(states, targetTensor, { epochs: 1, batchSize: this.BATCH_SIZE, verbose: 0 });

        // Epsilon decay and memory cleanup
        // ...
    }

    /**
     * Periodically synchronize the weights to the target network.
     */
    updateTargetModel() {
        this.targetModel.setWeights(this.model.getWeights());
    }
}</code></pre>
            </div>
        </section>

        <!-- Dueling Architecture Concept Section -->
        <section class="mb-12 sm:mb-16 mt-12">
            <div class="pb-2 mb-6 border-b-2 border-gray-800">
                <h2 class="text-2xl sm:text-3xl font-bold">
                    Dueling Network Architecture Concept
                </h2>
            </div>

            <p class="text-base sm:text-lg text-gray-700 mb-6">
                The Dueling Network decomposes the Q-value into a State Value $V(s)$ and an Action Advantage $A(s, a)$. This separation allows the agent to learn which states are valuable independently of the action taken, improving sample efficiency.
            </p>

            <div class="code-block">
                <pre><code>// Conceptual Structure (Graph)
// The final Q-value is $Q(s, a) = V(s) + (A(s, a) - \frac{1}{|A|}\sum_{a'} A(s, a'))$

                      [Input State s]
                            |
                        [Shared Layers]
                            |
           +----------------+----------------+
           |                                |
       [Value Stream V(s)]           [Advantage Stream A(s, a)]
           |                                |
        [Output V(s)]               [Output A(s, a) vector]
           |                                |
           +----------------+----------------+
                            |
                        [Merging Layer] (Subtractive Averaging)
                            |
                         [Final Q(s, a)]
                         (Q-Values for all actions)</code></pre>
            </div>
        </section>

        <!-- Training Code Snippet Section (IPyNB) (NEW) -->
        <section class="mb-12 sm:mb-16 mt-12">
            <div class="pb-2 mb-6 border-b-2 border-gray-800">
                <h2 class="text-2xl sm:text-3xl font-bold">
                    Jupyter Notebook Training Initialization
                </h2>
            </div>
            <p class="text-base sm:text-lg text-gray-700 mb-6">
                The following cells initialize the PyTorch models, environment, and training components before starting the main $\texttt{train\_modif}$ loop.
            </p>
            <div class="code-block">
                <pre><code>import torch
import torch.optim as optim
from model import DQN
from wrappers import make_atari_env
from replay_memory import ReplayBuffer
from utils_modif import train_modif, test_modif
from Param_modif import *
from IPython.display import clear_output
import matplotlib.pyplot as plt
%matplotlib inline

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device
// Output: device(type='cuda')

env_id = "BoxingNoFrameskip-v4"
env = make_atari_env(env_id)
print(env.action_space)
// Output: Discrete(18)
// Output: ['NOOP', 'FIRE', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'UPRIGHT', 'UPLEFT', 'DOWNRIGHT', 'DOWNLEFT', 'UPFIRE', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'UPLEFTFIRE', 'DOWNRIGHTFIRE', 'DOWNLEFTFIRE']

current_model = DQN(env.observation_space.shape, env.action_space.n).to(device)
target_model = DQN(env.observation_space.shape, env.action_space.n).to(device)
optimizer = optim.Adam(current_model.parameters(), lr=0.0001)
replay_buffer = ReplayBuffer(MEMORY_SIZE)
target_model.load_state_dict(current_model.state_dict())
// Output: &lt;All keys matched successfully&gt;

train_modif(env, current_model,target_model, optimizer, replay_buffer, device)
</code></pre>
            </div>
        </section>


        <!-- DDQN Pseudo-code Section -->
        <section class="mb-12 sm:mb-16 mt-12">
            <div class="pb-2 mb-6 border-b-2 border-gray-800">
                <h2 class="text-2xl sm:text-3xl font-bold">
                    Double DQN Training Algorithm (Pseudo-code)
                </h2>
            </div>

            <p class="text-base sm:text-lg text-gray-700 mb-6">
                The Double DQN (DDQN) formula is used to calculate the target $Y_j$, which significantly reduces the overestimation bias inherent in classic DQN by decoupling the action selection from the target Q-value estimation.
            </p>

            <div class="code-block">
                <pre><code>Initialize Q-network Q with random weights $\theta$.
Initialize target Q-network Q' with weights $\theta' = \theta$.
Initialize Experience Replay buffer D (memory).

For each EPISODE:
  Initialize state $s$
  For $t = 1$ to $T$ (steps within the episode):

    // 1. Action Selection ($\epsilon$-Greedy Policy)
    Select action $a$:
      With probability $\epsilon$, choose random action $a$
      Otherwise, choose $a = \underset{a}{\operatorname{argmax}} Q(s, a; \theta)$

    // 2. Environment Interaction and Storage
    Execute action $a$, observe reward $r$, and new state $s'$
    Store experience $(s, a, r, s')$ in buffer D
    Set $s = s'$

    // 3. Training Step (Sampling and Optimization)
    Sample random mini-batch $(s_j, a_j, r_j, s'_j)$ from D

    // Calculate Target Value ($Y_j$) using Double DQN Update Rule
    // Select best action using MAIN Q-network: $\underset{a'}{\operatorname{argmax}} Q(s'_j, a'; \theta)$
    // Estimate Q-value of that action using TARGET Q-network: $Q'(s'_j, \text{action})$
    Calculate target value $y_j$:
      If $s'_j$ is terminal (game over):
        $y_j = r_j$
      Else (Double DQN Update):
        $a^* = \underset{a'}{\operatorname{argmax}} Q(s'_j, a'; \theta)$
        $y_j = r_j + \gamma \cdot Q'(s'_j, a^*; \theta')$


    // 4. Update Main Network
    Perform a gradient descent step on $(y_j - Q(s_j, a_j; \theta))^2$
    to update weights $\theta$ of the main Q-network.

    // 5. Update Target Network
    Every $C$ steps (e.g., $C=1000$):
      Update target network weights: $\theta' = \theta$

  Decay $\epsilon$ (e.g., $\epsilon = \epsilon \cdot \epsilon_{\text{decay}}$)</code></pre>
            </div>
        </section>

        <!-- Training Performance Graphs Section (NEW) -->
        <section class="mb-12 sm:mb-16 mt-12">
            <div class="pb-2 mb-6 border-b-2 border-gray-800">
                <h2 class="text-2xl sm:text-3xl font-bold">
                    Training Performance Metrics
                </h2>
            </div>
            <p class="text-base sm:text-lg text-gray-700 mb-6">
                The graphs below illustrate the agent's performance, comparing the reward curve (left) with the stability of the Q-value estimation (right) over training steps. 
            </p>

            <div class="grid grid-cols-1 md:grid-cols-2 gap-8">
                <!-- Graph 1: Reward History -->
                <div class="container-border rounded-lg overflow-hidden p-3 bg-gray-100">
                    <p class="text-sm font-semibold mb-2 text-center text-gray-800">EPISODIC REWARD HISTORY</p>
                    <img
                        src="totalframe.png"
                        onerror="this.onerror=null; this.src='https://placehold.co/600x350/9ca3af/1f2937?text=REWARD+VS+EPISODE+GRAPH';"
                        alt="Graph showing episodic reward history"
                        class="w-full h-auto object-cover container-border rounded"
                    >
                </div>

            
            </div>
        </section>

        <footer class="text-center pt-8 border-t-2 border-gray-400 mt-12">
            <p class="text-gray-600 font-mono text-xs sm:text-sm">A Reinforcement Learning Documentation Project | Built with Simplicity and Contrast.</p>
        </footer>
    </div>
</body>
</html>